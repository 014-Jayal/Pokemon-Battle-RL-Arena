# Pok√©mon Battle Arena: Reinforcement Learning Project

> **An AI Agent that learns to master Pok√©mon-style battles using Reinforcement Learning (PPO)**  

![Pokedex](assets/pokedex.png)

---

## Overview

This project implements a **Reinforcement Learning (RL)** framework where an AI agent learns to win **Pok√©mon battles** using **Proximal Policy Optimization (PPO)**.  
Built completely from scratch, it combines **custom environment design**, **type-based combat mechanics**, and **cinematic video visualization** of battles.

---

## Problem Statement

> Can an AI agent, without any pre-programmed rules, learn to play and win Pok√©mon battles purely through Reinforcement Learning?

This project builds a **custom Gymnasium environment** that allows an RL agent to learn **attack, defend, and heal strategies** based only on rewards and state transitions.

---

## System Architecture

![Architecture](assets/architecture.png)

---

## Key Features

‚úÖ **Custom RL Environment** ‚Äî Built entirely using `gymnasium`  
‚úÖ **PPO Agent (Stable-Baselines3)** ‚Äî Learns through self-play and reward optimization  
‚úÖ **Type-Based Combat System** ‚Äî Fire üî• > Grass üåø > Water üíß > Fire üî•  
‚úÖ **Dynamic Opponent Pool** ‚Äî Battles with random Pok√©mon for diverse learning  
‚úÖ **Visual Output** ‚Äî Generates `.mp4` battle replays for analysis  
‚úÖ **Reward Engineering** ‚Äî Balanced incentives for attack, defense, and healing  

---

## Methodology

1. **Custom Environment Creation**  
   - Defines HP, Energy, Actions, and Type Advantages  
   - Implements Gym-compatible reset and step functions  

2. **Agent Training (PPO)**  
   - Observes state `[HP, Energy, Opponent HP, Turn]`  
   - Takes actions: *Light Attack, Heavy Attack, Guard, Recover*  
   - Learns via reward feedback (damage dealt, win/loss)  

3. **Evaluation & Tournament Phase**  
   - Trained model battles multiple opponents (randomly selected)  
   - Results recorded and visualized as MP4 battle videos  

---

## Demo Battles

### Example Battle 1
![Battle Demo 1](assets/battle_demo_1.png)

### Example Battle 2
![Battle Demo 2](assets/battle_demo_2.png)

---

## Results

| Opponent | Type | Result |
|-----------|-------|--------|
| Charmander | Fire | ‚úÖ Win |
| Squirtle | Water | ‚ùå Loss |
| Bulbasaur | Grass | ‚úÖ Win |
| Pikachu | Electric | ‚úÖ Win |
| Gengar | Normal | ‚úÖ Win |

---

## Technologies Used

| Category | Tools |
|-----------|--------|
| Programming | Python 3.12 |
| Frameworks | Gymnasium, Stable-Baselines3 |
| Model | PPO (Proximal Policy Optimization) |
| Visualization | MoviePy, OpenCV, PIL |
| Environment | Google Colab / VS Code |
| Hardware | GPU / TPU Accelerator |

---

## How to Run

### 1Ô∏è‚É£ Clone Repository
```bash
git clone https://github.com/<your-username>/Pokemon-Battle-RL-Arena.git
cd Pokemon-Battle-RL-Arena
```

### 2Ô∏è‚É£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 3Ô∏è‚É£ Run the Notebook
Open `Pokemon_Battle_RL_Arena.ipynb` in Google Colab or VS Code Jupyter.

### 4Ô∏è‚É£ Train and Watch Battles
After training, run the **tournament cell** ‚Äî it will:
- Generate videos under `/battle_videos/`
- Display inline replay battles

---

## References

- Schulman et al., *Proximal Policy Optimization Algorithms*, OpenAI (2017)  
- Stable Baselines3 Documentation  
- Pok√©mon API Sprites Repository  
- OpenAI Gymnasium Framework  

---

## Team & Credits

**Authors:**  
- [Jayal Shah](https://www.linkedin.com/in/jayal-shah04/)  
- [Mayank Jangid](https://www.linkedin.com/in/mayank-jangid-0a5207359/)  
- [Dhairya Jadav](https://www.linkedin.com/in/dhairya-jadav-231217290/)  

**Mentor:** - [Palwinder Singh](https://www.linkedin.com/in/palwinder-singh-527960134/) 

---

## Future Scope
- Add advanced moves (e.g., status effects, elemental buffs)  
- Expand to 3D animation (Unity/Blender integration)  
- Include live Streamlit dashboard for real-time battle visualizations  

---

‚≠ê **If you liked this project, give it a star on GitHub!**
